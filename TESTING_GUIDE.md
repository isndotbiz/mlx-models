# Speculative Decoding Testing Guide

## Quick Start

```bash
# Run the complete test suite
python test_speculative_complete.py
```

## What Gets Tested

### 1. Server Status Check
- âœ… Speculative server (port 8000)
- âœ… LM Studio (port 1234)
- âœ… MLX Server (port 11434)

### 2. Configuration Testing
Tests all speculative decoding configurations:
- **Baseline**: No speculative decoding (reference)
- **3 tokens**: Light speculative decoding
- **5 tokens**: Moderate speculative decoding (usually optimal)
- **7 tokens**: Aggressive speculative decoding

**Metrics measured**:
- Tokens per second
- Total generation time
- Speedup percentage vs baseline
- Identifies best configuration automatically

### 3. Prompt Type Testing
Tests how different tasks benefit from speculative decoding:
- **Security research**: Technical query about vulnerabilities
- **Code generation**: Function implementation with types
- **Technical explanation**: Detailed ML/AI concepts
- **Creative problem-solving**: Open-ended reasoning

**Shows which tasks get biggest speedup!**

### 4. System Prompt Testing
Compares different system prompts:
- **Anti-refusal**: Direct, no-refusal prompt
- **Empty**: No system prompt
- **Expert**: Expert assistant prompt
- **Concise**: Brief response prompt

Tests both performance and output quality.

### 5. Report Generation
Creates `test_results.json` with:
- All performance metrics
- Speedup calculations
- Best configuration recommendation
- Best use case identification
- Detailed timing breakdowns

## Understanding the Output

### Server Status
```
Server Status Check
===================
âœ… Speculative Server (port 8000): RUNNING
âœ… LM Studio (port 1234): RUNNING
âŒ MLX Server (port 11434): NOT RUNNING - Connection refused
```

### Performance Comparison
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Speculative Decoding Performance            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¤
â”‚ Config       â”‚ Tok/Sec  â”‚ Time   â”‚ Speedup â”‚ Best â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ No spec      â”‚    18.2  â”‚ 13.74s â”‚ baselineâ”‚      â”‚
â”‚ 3 tokens     â”‚    24.1  â”‚ 10.37s â”‚  +32%   â”‚      â”‚
â”‚ 5 tokens     â”‚    29.8  â”‚  8.39s â”‚  +64%   â”‚ â­   â”‚
â”‚ 7 tokens     â”‚    27.3  â”‚  9.16s â”‚  +50%   â”‚      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
```

### Prompt Type Comparison
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Performance by Prompt Type               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Type         â”‚ Baseline â”‚ Speculativeâ”‚ Speedup â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Security     â”‚   18.5   â”‚    28.2    â”‚  +53%   â”‚
â”‚ Code gen     â”‚   19.1   â”‚    31.4    â”‚  +64%   â”‚
â”‚ Technical    â”‚   17.8   â”‚    29.8    â”‚  +67%   â”‚
â”‚ Creative     â”‚   18.9   â”‚    27.1    â”‚  +43%   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Recommendations Section

The test automatically identifies:

1. **Best Configuration**: Optimal number of draft tokens
2. **Best Use Case**: Which prompt type benefits most
3. **Performance Gains**: Expected speedup for your workload

Example:
```
ğŸ† Recommended Configuration: 5 draft tokens
   Performance: 29.8 tokens/sec
   Speedup: +64% vs baseline

ğŸ’¡ Best Use Case: Technical explanation
   Achieves +67% speedup with speculative decoding
```

## Installation

### Required
```bash
pip install requests
```

### Optional (for better visuals)
```bash
pip install rich
```

The script works without `rich`, but you get:
- âœ¨ Beautiful tables
- ğŸ¨ Color-coded output
- ğŸ“Š Progress indicators
- ğŸ“¦ Formatted panels

## What to Look For

### Good Results
- âœ… Speedup of 30-70% with speculative decoding
- âœ… 5 tokens usually optimal
- âœ… Bigger models benefit more
- âœ… Repetitive tasks see biggest gains

### Warning Signs
- âš ï¸ Negative speedup (speculative slower than baseline)
- âš ï¸ Server connection errors
- âš ï¸ Very low tokens/sec (< 10 tok/s)

### Troubleshooting

**Server not responding?**
```bash
# Check if running
lsof -i :8000

# Restart server
python test_speculative.py
```

**Slow performance?**
- Check CPU/GPU usage
- Close other applications
- Try smaller draft model
- Reduce number of draft tokens

**Connection errors?**
- Verify server is running
- Check firewall settings
- Ensure correct port numbers

## Advanced Usage

### Test specific server only
Edit the script to change default server:
```python
config_results = suite.test_speculative_configurations('lm_studio')
```

### Test with different prompts
Add custom prompts to `test_prompts` dictionary:
```python
self.test_prompts['custom'] = {
    'prompt': "Your custom prompt here",
    'description': "Custom test",
    'expected_tokens': 200
}
```

### Change output file
```python
suite.generate_report(results, output_file="my_results.json")
```

## Interpreting Results

### Speedup Patterns

**High speedup (50-70%)**:
- Predictable text generation
- Code completion
- Templated responses
- Technical documentation

**Moderate speedup (30-50%)**:
- General Q&A
- Mixed content types
- Varied vocabulary

**Low speedup (0-30%)**:
- Highly creative tasks
- Unpredictable outputs
- Very short responses

### Optimal Configuration

Most common results:
- **3 tokens**: 20-35% speedup, low overhead
- **5 tokens**: 40-65% speedup, best balance â­
- **7 tokens**: 30-55% speedup, higher overhead

**Rule of thumb**: Start with 5, adjust based on results.

## Next Steps

After running tests:

1. **Check `test_results.json`** for detailed metrics
2. **Note the recommended configuration**
3. **Update your server startup** with optimal settings
4. **Test with your actual workload**
5. **Monitor performance in production**

## Production Deployment

Use the optimal settings found:

```bash
# Start with recommended config (usually 5 tokens)
python -m vllm.entrypoints.openai.api_server \
    --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B \
    --speculative-model Qwen/Qwen2.5-1.5B-Instruct \
    --num-speculative-tokens 5 \
    --use-v2-block-manager \
    --disable-log-requests
```

## Continuous Testing

Re-run tests when:
- âœ… Updating models
- âœ… Changing hardware
- âœ… Modifying configuration
- âœ… Testing new prompts
- âœ… Validating performance

Keep `test_results.json` for historical comparison!

## Support

If you encounter issues:
1. Check server logs
2. Verify model downloads
3. Test baseline (no speculative) first
4. Compare with test results
5. Check system resources

Happy testing! ğŸš€
